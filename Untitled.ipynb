{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as utils\n",
    "# from ctcdecode import CTCBeamDecoder\n",
    "# from warpctc_pytorch import CTCLoss\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhonemeDataset(Dataset):\n",
    "    def __init__(self, path, name):\n",
    "        self.data = np.load(os.path.join(path, '{}.npy'.format(name)), encoding='bytes')\n",
    "        self.labels = np.load(os.path.join(path, '{}_labels.npy'.format(name)), encoding='bytes')\n",
    "        # save dimensions of data\n",
    "        self.length = self.data.shape[0]\n",
    "        self.seq_lens = [seq.shape[0] for seq in self.data]\n",
    "        self.lab_lens = [lab.shape[0] for lab in self.labels]\n",
    "        self.seq_len = max(self.seq_lens)\n",
    "        self.lab_len = max(self.lab_lens)\n",
    "        # tensorize\n",
    "        self.data = [torch.Tensor(d) for d in self.data]\n",
    "        self.labels = [torch.LongTensor(l) for l in self.labels]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i], self.labels[i]\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate(seq_list):\n",
    "        inputs, targets = zip(*seq_list)\n",
    "        lens = [len(seq) for seq in inputs]\n",
    "        seq_order = sorted(range(len(lens)), key=lens.__getitem__, reverse=True)\n",
    "        inputs = [inputs[i] for i in seq_order]\n",
    "        targets = [targets[i] for i in seq_order]\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = PhonemeDataset('data', 'train')\n",
    "vs = PhonemeDataset('data', 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = DataLoader(ts, batch_size=64, shuffle=True, collate_fn=TrainDataset.collate)\n",
    "vl = DataLoader(vs, batch_size=64, shuffle=True, collate_fn=TrainDataset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Baseline Model\n",
    "        - 3 stacked BiLSTM layers, each of 256 units.\n",
    "        - 1 Dense layer of 47 (num_classes) units.\n",
    "        - Adam optimizer with default learning rate of 1e-3\n",
    "        - Decoding with beam search and a beam width of 100.\n",
    "        \"\"\"\n",
    "        super(BaselineModel, self).__init__()\n",
    "\n",
    "        # save parameters\n",
    "        self.n_layers = 3\n",
    "        self.input_size = 40\n",
    "        self.hidden_size = 256\n",
    "        self.output_size = 46\n",
    "\n",
    "        # layers\n",
    "        self.recurrent = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.n_layers, bias=False, bidirectional=True)\n",
    "        self.scoring = nn.Linear(2*self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, seq_list):\n",
    "        \"\"\"Forward pass in the model.\n",
    "        Takes:\n",
    "        - seq_batch: list of length B, with tensors of dim L x 40 (L differs)\n",
    "        Returns:\n",
    "        - output: list of length B, with tensors of dim L x V (L differs)\n",
    "        Dimensions:\n",
    "        - L: sequence length\n",
    "        - B: batch size\n",
    "        - H: hidden size\n",
    "        - V: vocab size\n",
    "        \"\"\"\n",
    "        # packed padded sequence\n",
    "        batch_size = len(seq_list)\n",
    "        lengths = [len(s) for s in seq_list]\n",
    "        print(lengths)\n",
    "        print(sum(lengths))\n",
    "        packed_input = utils.rnn.pack_sequence(seq_list)\n",
    "        print('packed input', packed_input.data.size())\n",
    "        # LSTM - embedded data as L x B\n",
    "        hidden = None\n",
    "        output_packed, hidden = self.recurrent(packed_input, hidden)\n",
    "        print('output packed', output_packed.data.size())\n",
    "        output_padded, _ = utils.rnn.pad_packed_sequence(output_packed)\n",
    "        print('output',output_padded.shape)\n",
    "        output_flatten = torch.cat([output_padded[:lengths[i], i] for i in range(batch_size)])\n",
    "        # full linear layer for scoring\n",
    "        print(output_flatten.shape)\n",
    "        print(batch_size)\n",
    "        output_scoring = self.scoring(output_flatten)\n",
    "        print(output_scoring.size())\n",
    "        output_unflatten = [output_scoring[sum(lengths[:i]) : sum(lengths[:i+1]), :] for i in range(batch_size)]\n",
    "        return output_unflatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [2,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, [2, 2])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lengths[:1]), lengths[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "  def __init__(self, model, train_loader, val_loader, epochs, run_id='exp'):\n",
    "    self.model = model\n",
    "    self.run_id = run_id\n",
    "    self.epochs = 0\n",
    "    self.n_epochs = epochs\n",
    "    self.val_loader = val_loader\n",
    "    self.train_loader = train_loader\n",
    "    self.gpu = torch.cuda.is_available()\n",
    "    self.val_losses = []\n",
    "    self.train_losses = []\n",
    "\n",
    "    if self.gpu: \n",
    "      self.model = self.model.cuda()\n",
    "    \n",
    "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "#     self.criterion = CTCLoss()\n",
    "  \n",
    "  def train(self):\n",
    "    self.model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
    "      print('\\rBatch {:03}/{:03}'.format(batch_idx+1, len(self.train_loader)), end='')\n",
    "      epoch_loss += self.train_batch(inputs, targets)\n",
    "    \n",
    "    epoch_loss = epoch_loss / len(self.train_loader)\n",
    "\n",
    "    self.train_losses.append(epoch_loss)\n",
    "\n",
    "    self.epochs += 1\n",
    "\n",
    "    print('\\r[TRAIN] Epoch {:02}/{:02} Loss {:7.4f}'.format(\n",
    "      self.epochs, self.n_epochs, epoch_loss\n",
    "    ), end='\\t')\n",
    "  \n",
    "  def train_batch(self, inputs, targets):\n",
    "    if self.gpu:\n",
    "      inputs = inputs.cuda()\n",
    "      targets = targets.cuda()\n",
    "    \n",
    "    outputs = self.model(inputs)\n",
    "#     print('raw output', len(outputs), outputs[0].size(), outputs[1].size())\n",
    "    output_sizes = [out.size(0) for out in outputs]\n",
    "    outputs = utils.rnn.pad_sequence(outputs)\n",
    "#     print('padded output',outputs.size())\n",
    "#     targets = targets\n",
    "#     .view(-1).type(torch.LongTensor)\n",
    "    target_sizes = [tar.size(0) for tar in targets]\n",
    "    target_sizes = torch.LongTensor(target_sizes)\n",
    "    targets = torch.cat(targets)\n",
    "#     print('padded targets', targets.size())\n",
    "#     print('target sizes:', len(target_sizes))\n",
    "#     print('target sizes sum:', sum(target_sizes))\n",
    "\n",
    "    loss = self.criterion(outputs, targets, output_sizes, target_sizes)\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    return loss.detach().cpu().item()\n",
    "  \n",
    "  def save(self):\n",
    "    torch.save(\n",
    "      {'state_dict': self.model.state_dict()},\n",
    "      os.path.join('experiments', self.run_id, 'model_{}.pkl'.format(self.epochs))\n",
    "    )\n",
    "    with open(os.path.join('experiments', self.run_id, 'train_losses.txt'), 'w') as fw:\n",
    "      for i in range(len(self.train_losses)):\n",
    "        fw.write('{:02} {:10.6}\\n'.format(i, self.train_losses[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, tl, vl, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 001/387[1248, 1247, 1146, 1055, 1036, 1020, 998, 984, 981, 907, 905, 896, 883, 856, 850, 832, 827, 826, 825, 824, 768, 758, 747, 739, 732, 699, 697, 687, 675, 657, 613, 611, 607, 597, 587, 572, 555, 554, 543, 517, 501, 485, 479, 478, 475, 468, 461, 451, 449, 442, 434, 421, 411, 407, 404, 381, 362, 337, 330, 301, 270, 250, 206, 149]\n",
      "41413\n",
      "packed input torch.Size([41413, 40])\n",
      "output packed torch.Size([41413, 512])\n",
      "output torch.Size([1248, 64, 512])\n",
      "torch.Size([41413, 512])\n",
      "64\n",
      "torch.Size([41413, 46])\n",
      "raw output 64 torch.Size([1248, 46]) torch.Size([1247, 46])\n",
      "padded output torch.Size([1248, 64, 46])\n",
      "padded targets torch.Size([5034])\n",
      "target sizes: 64\n",
      "target sizes sum: 5034\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1248) to match target batch_size (5034).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-166-a450a2623c7c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\rBatch {:03}/{:03}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-166-a450a2623c7c>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target sizes sum:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 862\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1405\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1248) to match target batch_size (5034)."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.load(os.path.join('data', 'train_labels.npy'), encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([l.shape[0] for l in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [torch.LongTensor(lab) for lab in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = utils.rnn.pad_sequence(labels, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24724, 210])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
